#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
from konlpy.tag import Mecab 

# In[2]:


# pip install --upgrade pandas

# In[3]:


# pip install --upgrade pip 

# In[4]:


# pip install konlpy 

# In[5]:


# pip install mecab_python-0.996_ko_0.9.2_msvc-cp38-cp38-win_amd64.whl

# In[6]:


#pip install --upgrade pip

# In[7]:


# pip install JPype1-1.4.0-cp39-cp39-win_amd64.whl

# In[8]:


pip install mecab_python-0.996_ko_0.9.2_msvc-cp38-cp38-win_amd64.whl

# In[9]:


from konlpy.tag import Komoran 
komoran = Komoran() 
text = "ì•„ë²„ì§€ê°€ ë°©ì— ë“¤ì–´ê°€ì‹ ë‹¤." 
komoran.nouns(text)
komoran.morphs(text)

# In[40]:


import MeCab

m = MeCab.Tagger()
out = m.parse("ì•„ë¹ ê°€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤.")

print(out)

# In[41]:


import MeCab

m = MeCab.Tagger()
out = m.parse("íŒŒì´ì¬ ì—ì„œ í˜•íƒœì†Œ ë¶„ì„í•˜ê¸°.")

print(out)

# In[36]:


import re

# Basic Cleaning Text Function
def CleanText(readData, Num=False, Eng=False):

    # Remove Retweets 
    text = re.sub('RT @[\w_]+: ', '', readData)

    # Remove Mentions
    text = re.sub('@[\w_]+', '', text)

    # Remove or Replace URL 
    text = re.sub(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", ' ', text) # httpë¡œ ì‹œì‘ë˜ëŠ” url
    text = re.sub(r"[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{2,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)", ' ', text) # httpë¡œ ì‹œì‘ë˜ì§€ ì•ŠëŠ” url
    
    # Remove Hashtag
    text = re.sub('[#]+[0-9a-zA-Z_]+', ' ', text)

    # Remove Garbage Words (ex. &lt, &gt, etc)
    text = re.sub('[&]+[a-z]+', ' ', text)

    # Remove Special Characters
    text = re.sub('[^0-9a-zA-Zã„±-ã…ê°€-í£]', ' ', text)
    
    # Remove newline
    text = text.replace('\n',' ')
    
    if Num is True:
        # Remove Numbers
        text = re.sub(r'\d+',' ',text)
    
    if Eng is True:
        # Remove English 
        text = re.sub('[a-zA-Z]' , ' ', text)

    # Remove multi spacing & Reform sentence
    text = ' '.join(text.split())
       
    return text

# In[37]:


SAMPLE_TEXT = "<ğŸš¨ì½”ë¡œë‚˜19 ê°€ì§œë‰´ìŠ¤ íŒ©íŠ¸ì²´í¬>\n\nì‹ ì²œì§€ê°€ ê¸°ì„±êµíšŒì— ê°€ì„œ ì½”ë¡œë‚˜ë¥¼ ì „íŒŒí•˜ë¼ê³  í–ˆë‹¤??!\n\n- ì‚¬ì‹¤ ë¬´ê·¼ì…ë‹ˆë‹¤. \nì‹ ì²œì§€ëŠ” 2ì›” 18ì¼ ë¶€í„° ì „êµ­ êµíšŒë¥¼ íì‡„í•˜ê³  ì˜¨ë¼ì¸\nì˜ˆë°°ë¡œ ì „í™˜í•˜ì˜€ìŠµë‹ˆë‹¤.\n\n#ì˜¨ë¼ì¸ì˜ˆë°°\n#ê°€ì§œë‰´ìŠ¤_ì´ì œê·¸ë§Œ\n#ì‹ ì²œì§€_íŒ©íŠ¸ì²´í¬ pic.twitter.com/Dppie6iean"

print(f"Before cleaning text:\n{SAMPLE_TEXT}")
print("\n")
print(f"After cleaning text:\n{CleanText(SAMPLE_TEXT)}")
print("\n")
print(f"After cleaning text when Num is True:\n{CleanText(SAMPLE_TEXT, Num=True)}")

# In[34]:


from konlpy.tag import Mecab 

# Preprocessing code with Mecab
mecab = Mecab(dicpath="C:\mecab\mecab-ko-dic") # Mecab User Dic Path

def preprocessing_mecab(readData):
    
    #### Clean text
    sentence = CleanText(readData)
    
    #### Tokenize
    morphs = mecab.pos(sentence)
    
    JOSA = ["JKS", "JKC", "JKG", "JKO", "JKB", "JKV", "JKQ", "JX", "JC"] # ì¡°ì‚¬
    SIGN = ["SF", "SE", "SSO", "SSC", "SC", "SY"] # ë¬¸ì¥ ë¶€í˜¸
    TERMINATION = ["EP", "EF", "EC", "ETN", "ETM"] # ì–´ë¯¸
    SUPPORT_VERB = ["VX"] # ë³´ì¡° ìš©ì–¸
    NUMBER = ["SN"]
    
    # Remove JOSA, EOMI, etc
    morphs[:] = (morph for morph in morphs if morph[1] not in JOSA+SIGN+TERMINATION+SUPPORT_VERB)
        
    # Remove length-1 words  
    morphs[:] = (morph for morph in morphs if not (len(morph[0]) == 1))
    
    # Remove Numbers
    morphs[:] = (morph for morph in morphs if morph[1] not in NUMBER)
   
    # Result pop-up
    result = []
    for morph in morphs:
        result.append(morph[0])
        
    return result

# In[38]:


SAMPLE_TEXT = "<ğŸš¨ì½”ë¡œë‚˜19 ê°€ì§œë‰´ìŠ¤ íŒ©íŠ¸ì²´í¬>\n\nì‹ ì²œì§€ê°€ ê¸°ì„±êµíšŒì— ê°€ì„œ ì½”ë¡œë‚˜ë¥¼ ì „íŒŒí•˜ë¼ê³  í–ˆë‹¤??!\n\n- ì‚¬ì‹¤ ë¬´ê·¼ì…ë‹ˆë‹¤. \nì‹ ì²œì§€ëŠ” 2ì›” 18ì¼ ë¶€í„° ì „êµ­ êµíšŒë¥¼ íì‡„í•˜ê³  ì˜¨ë¼ì¸\nì˜ˆë°°ë¡œ ì „í™˜í•˜ì˜€ìŠµë‹ˆë‹¤.\n\n#ì˜¨ë¼ì¸ì˜ˆë°°\n#ê°€ì§œë‰´ìŠ¤_ì´ì œê·¸ë§Œ\n#ì‹ ì²œì§€_íŒ©íŠ¸ì²´í¬ pic.twitter.com/Dppie6iean"

# RUN!
preprocessing_mecab(SAMPLE_TEXT)

print(out)

# In[43]:


from konlpy.tag import Kkma
kkma = Kkma()	# ì•„ë§ˆ ì„¤ì¹˜ê°€ ì˜ ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ê°€ ë‚¬ì„ ê²ƒì´ë‹¤.
print(kkma.nouns(u'ì•ˆë…•í•˜ì„¸ìš” Sooì…ë‹ˆë‹¤'))

# In[44]:


from konlpy.tag import Okt
okt = Okt()

# In[45]:


#ì „ì²˜ë¦¬ê³¼ì •
df = pd.read_csv('gsí¸ì˜ì _2022(ëŒ€cvs)_1.csv')
df.head()

# In[46]:


df["Context"] = df["Context"].str.replace(pat=r'[^\w]', repl=r'', regex=True)
#íŠ¹ìˆ˜ë¬¸ìì œê±°

# In[47]:


df["Context"] = df["Context"].str.replace(pat=r'["_"]', repl=r'', regex=True)
#_ ì œê±°

# In[50]:


df["Context"] = df["Context"].str.replace(pat=r'["^a-zA-Z0-9"]', repl=r'', regex=True)
# ì˜ì–´,ìˆ«ìì œê±°

# In[51]:


df.head()

# In[53]:


# í•œì¤„ë¡œ ë¬¶ê¸°
context = df['Context'].tolist()
print(len(context))

context=' '.join(context)
context=context[:]
print(context)

# In[54]:


!pip install matplotlib-venn

# In[55]:


from konlpy.tag import Okt
okt = Okt()

# In[56]:


tokenizer = Okt()
raw_pos_tagged = tokenizer.pos(context, norm=True, stem=True) # POS Tagging
print(raw_pos_tagged)

# In[57]:


#ë¶ˆìš©ì–´ ì²˜ë¦¬ ë¦¬ìŠ¤íŠ¸

# In[58]:


del_list = ['ë¥¼', 'ì´', 'ì€', 'ëŠ”', 'ìˆë‹¤', 'í•˜ë‹¤', 'ì—']  

word_cleaned = []
for word in raw_pos_tagged:
    if not word[1] in ["Josa", "Eomi", "Punctuation", "Foreign"]: # Foreign == â€, â€œ ì™€ ê°™ì´ ì œì™¸ë˜ì–´ì•¼í•  í•­ëª©ë“¤
        if (len(word[0]) != 1) & (word[0] not in del_list): # í•œ ê¸€ìë¡œ ì´ë¤„ì§„ ë‹¨ì–´ë“¤ì„ ì œì™¸ & ì›ì¹˜ ì•ŠëŠ” ë‹¨ì–´ë“¤ì„ ì œì™¸, ëŒ€ì‹  "ì•ˆ, ëª»"ê°™ì€ ê²ƒê¹Œì§€ ê°™ì´ ì§€ì›Œì ¸ì„œ ê¸ì •,ë¶€ì •ì„ íŒŒì•…í•´ì•¼ ë˜ëŠ”ê²½ìš°ëŠ” ì œì™¸í•˜ì§€ ì•ŠëŠ”ë‹¤.
            word_cleaned.append(word[0])
        
print(word_cleaned)

# In[59]:


from collections import Counter

# In[60]:


result = Counter(word_cleaned)
word_dic = dict(result)
print(word_dic)

# In[61]:


#ì¹´ìš´í„° ê¸°ë³¸ ì‚¬ìš©ë²•
#https://www.daleseo.com/python-collections-counter/

# In[62]:


# # ê·¸í›„ sorted( ) í•¨ìˆ˜ë¡œ ì •ë ¬í•´ì¤ë‹ˆë‹¤. keyíŒŒë¼ë¯¸í„°ì— ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì „ë‹¬ì¸ìë¡œ í•˜ëŠ” lambdaí•¨ìˆ˜ë¥¼ í†µí•´ ì •ë ¬í•´ì¤ë‹ˆë‹¤.

# word_dic.imtes( )ë¥¼ í†µí•´ ë”•ì…”ë„ˆë¦¬ì˜ key, valueìŒì„ íŠœí”Œë¡œ ë°›ìŠµë‹ˆë‹¤.
# ì´ íŠœí”Œì—ì„œ lambdaí•¨ìˆ˜ì— outputê°’ìœ¼ë¡œ x[1]ì„ ì„¤ì •í•˜ì—¬ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ keyíŒŒë¼ë¯¸í„°ì˜ ê°’ìœ¼ë¡œ í•©ë‹ˆë‹¤.

# In[63]:


sorted_word_dic = sorted(word_dic.items(), key=lambda x:x[1], reverse=True)
print(sorted_word_dic)

# In[67]:


import nltk
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import font_manager, rc

# ê·¸ë˜í”„ì— í•œê¸€ í°íŠ¸ ì„¤ì •
font_name = matplotlib.font_manager.FontProperties(fname="C:/Windows/Fonts/malgun.ttf").get_name() # NanumGothic.otf
matplotlib.rc('font', family=font_name)


word_counted = nltk.Text(word_cleaned)
plt.figure(figsize=(15,7))
plt.title('íŠ¸ìœ„í„° ê²€ìƒ‰ì–´: -GSí¸ì˜ì -ê¸€ ì—ì„œ ì“°ì¸ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ')

word_counted.plot(50)



# In[66]:


# pip install nltk

# In[68]:


word_frequency = nltk.FreqDist(word_cleaned)
df = pd.DataFrame(list(word_frequency.values()), word_frequency.keys())

result = df.sort_values([0], ascending = False)
result = result[:50]
result.plot(kind='bar', legend=False, figsize=(15,5))
plt.title('íŠ¸ìœ„í„° ê²€ìƒ‰ì–´: -GSí¸ì˜ì -ê¸€ ì—ì„œ ì“°ì¸ ë‹¨ì–´ ë¹ˆë„ìˆ˜')
plt.show()

# In[ ]:




# In[69]:


from wordcloud import WordCloud 
import matplotlib.pyplot as plt
import numpy as np
from PIL import *

# In[14]:


# pip install wordcloud

# In[70]:


cand_mask=np.array(Image.open('circle.jpg'))

# In[80]:


cand_mask=np.array(Image.open('circle.jpg'))

# In[94]:


sorted_word_dic

# In[99]:


#ë°°ì—´ê°’ ë©”ëª¨ì¥ì— ë„£ê¸°

# In[103]:


# pip install numpy

# In[104]:


import numpy as np

# In[111]:


f = open("sorted_word_dic.txt",'w')
f.write(str(sorted_word_dic))
f.close()

# In[112]:


from operator import itemgetter

# In[116]:


print(sorted_word_dic[:5])

# In[136]:


word_dict ={}
for n, i in sorted_word_dic[:10]:
    word_dict[n]=i
    
    print("í¸ì˜ì  wordcount:",word_dict['í¸ì˜ì '])
    print("ë”•ì…”ë„ˆë¦¬ê°¯ìˆ˜:",len(word_dict))

# In[134]:


wc = WordCloud(font_path = "malgun",
        max_words = 100,
        max_font_size = 200)
cloud = wc.fit_words(word_dict)
cloud.to_image() # ìƒìœ„ 20ê°œ ë‹¨ì–´


# In[137]:


wc = WordCloud(font_path = "malgun",
        max_words = 100,
        max_font_size = 200)
cloud = wc.fit_words(word_dict)
cloud.to_image() #ìƒìœ„ 10 ë‹¨ì–´


# In[ ]:



